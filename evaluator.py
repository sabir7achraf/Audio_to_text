 # reading_evaluator.py
import google.generativeai as genai
import json
import difflib
import re
from typing import Dict, List, Any
import os
from dataclasses import dataclass
from enum import Enum

class ReadingLevel(Enum):
    EXCELLENT = "Ù…Ù…ØªØ§Ø²"
    VERY_GOOD = "Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹"
    GOOD = "Ø¬ÙŠØ¯"
    ACCEPTABLE = "Ù…Ù‚Ø¨ÙˆÙ„"
    NEEDS_IMPROVEMENT = "ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†"
    POOR = "Ø¶Ø¹ÙŠÙ"

@dataclass
class ReadingEvaluation:
    overall_score: float
    level: ReadingLevel
    pronunciation_score: float
    fluency_score: float
    accuracy_score: float
    comprehension_score: float
    feedback: str
    detailed_feedback: Dict[str, Any]
    suggestions: List[str]
    strengths: List[str]
    areas_to_improve: List[str]

class ArabicReadingEvaluator:
    def __init__(self, api_key: str = None):
        """
        Initialize the Arabic Reading Evaluator
        
        Args:
            api_key: Google Gemini API key. If None, will try to get from environment variable
        """
        self.api_key = api_key or os.getenv('GEMINI_API_KEY')
        if not self.api_key:
            raise ValueError("Gemini API key is required. Set GEMINI_API_KEY environment variable or pass it directly.")
        
        # Configure Gemini
        genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel('gemini-1.5-flash')  # Free tier model
        
        # Arabic text processing patterns
        self.diacritics_pattern = re.compile(r'[\u064B-\u0652\u0670\u0640]')  # Arabic diacritics and tatweel
        self.arabic_letters_pattern = re.compile(r'[\u0621-\u063A\u0641-\u064A]')  # Arabic letters
    
    def normalize_arabic_text(self, text: str) -> str:
        """Normalize Arabic text for comparison"""
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text.strip())
        
        # Normalize some common variations
        text = text.replace('Ø£', 'Ø§').replace('Ø¥', 'Ø§').replace('Ø¢', 'Ø§')
        text = text.replace('Ø©', 'Ù‡')
        text = text.replace('Ù‰', 'ÙŠ')
        
        return text
    
    def remove_diacritics(self, text: str) -> str:
        """Remove Arabic diacritics from text"""
        return self.diacritics_pattern.sub('', text)
    
    def calculate_accuracy_score(self, transcription: str, original_text: str) -> Dict[str, Any]:
        """Calculate detailed accuracy metrics"""
        # Normalize both texts
        norm_transcription = self.normalize_arabic_text(transcription)
        norm_original = self.normalize_arabic_text(original_text)
        
        # Remove diacritics for comparison
        clean_transcription = self.remove_diacritics(norm_transcription)
        clean_original = self.remove_diacritics(norm_original)
        
        # Word-level comparison
        trans_words = clean_transcription.split()
        orig_words = clean_original.split()
        
        # Use sequence matcher for similarity
        seq_matcher = difflib.SequenceMatcher(None, trans_words, orig_words)
        word_similarity = seq_matcher.ratio()
        
        # Character-level comparison
        char_matcher = difflib.SequenceMatcher(None, clean_transcription, clean_original)
        char_similarity = char_matcher.ratio()
        
        # Calculate word accuracy
        correct_words = 0
        total_words = len(orig_words)
        
        for i, orig_word in enumerate(orig_words):
            if i < len(trans_words):
                word_sim = difflib.SequenceMatcher(None, orig_word, trans_words[i]).ratio()
                if word_sim >= 0.8:  # Consider 80% similarity as correct
                    correct_words += 1
        
        word_accuracy = (correct_words / total_words * 100) if total_words > 0 else 0
        
        return {
            "word_accuracy": word_accuracy,
            "word_similarity": word_similarity * 100,
            "character_similarity": char_similarity * 100,
            "total_words_original": total_words,
            "total_words_transcribed": len(trans_words),
            "correct_words": correct_words,
            "missing_words": max(0, total_words - len(trans_words)),
            "extra_words": max(0, len(trans_words) - total_words)
        }
    
    def create_evaluation_prompt(self, transcription: str, original_text: str, accuracy_metrics: Dict) -> str:
        """Create a comprehensive prompt for LLM evaluation"""
        
        prompt = f"""
Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ØªÙ‚ÙŠÙŠÙ… Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„Ø·Ù„Ø§Ø¨. Ù‚Ù… Ø¨ØªÙ‚ÙŠÙŠÙ… Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø·Ø§Ù„Ø¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ ÙˆØ§Ù„Ù†Øµ Ø§Ù„Ù…Ù†Ø·ÙˆÙ‚.

Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ:
{original_text}

Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù†Ø·ÙˆÙ‚ (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„ØµÙˆØªÙŠØ©):
{transcription}

Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:
1. Ø§Ù„Ù†Ø·Ù‚ ÙˆØ§Ù„ÙˆØ¶ÙˆØ­ (Pronunciation): Ù…Ø¯Ù‰ ÙˆØ¶ÙˆØ­ Ù†Ø·Ù‚ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙˆØ§Ù„Ø­Ø±ÙˆÙ
2. Ø§Ù„Ø·Ù„Ø§Ù‚Ø© (Fluency): Ø³Ù„Ø§Ø³Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© ÙˆØ³Ø±Ø¹ØªÙ‡Ø§ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
3. Ø§Ù„Ø¯Ù‚Ø© (Accuracy): Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚Ø±ÙˆØ¡ Ù„Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ
4. Ø§Ù„ÙÙ‡Ù… (Comprehension): ÙÙ‡Ù… Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ù†Øµ

Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ©:
- Ø¯Ù‚Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {accuracy_metrics['word_accuracy']:.1f}%
- ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {accuracy_metrics['word_similarity']:.1f}%
- ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø£Ø­Ø±Ù: {accuracy_metrics['character_similarity']:.1f}%
- Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©: {accuracy_metrics['total_words_original']}
- Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù†Ø·ÙˆÙ‚Ø©: {accuracy_metrics['total_words_transcribed']}
- Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©: {accuracy_metrics['correct_words']}
- Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©: {accuracy_metrics['missing_words']}
- Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©: {accuracy_metrics['extra_words']}

Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù…Ù†Ùƒ:
1. ØªÙ‚ÙŠÙŠÙ… Ø´Ø§Ù…Ù„ Ù„Ù„Ù‚Ø±Ø§Ø¡Ø© Ù…Ø¹ Ø¯Ø±Ø¬Ø© Ù…Ù† 100 Ù„ÙƒÙ„ Ù…Ø¹ÙŠØ§Ø±
2. ØªØ­Ø¯ÙŠØ¯ Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ© ÙÙŠ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©
3. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø³ÙŠÙ†
4. ØªÙ‚Ø¯ÙŠÙ… Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù…Ø­Ø¯Ø¯Ø© Ù„Ù„ØªØ­Ø³ÙŠÙ†
5. ØªÙ‚Ø¯ÙŠÙ… ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø© Ù…Ø´Ø¬Ø¹Ø© ÙˆÙ…ÙÙŠØ¯Ø©

ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø±Ø¯ Ø¨ØµÙŠØºØ© JSON Ø¨Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„ØªØ§Ù„ÙŠ:
{{
    "pronunciation_score": Ø¯Ø±Ø¬Ø© Ù…Ù† 100,
    "fluency_score": Ø¯Ø±Ø¬Ø© Ù…Ù† 100,
    "accuracy_score": Ø¯Ø±Ø¬Ø© Ù…Ù† 100,
    "comprehension_score": Ø¯Ø±Ø¬Ø© Ù…Ù† 100,
    "overall_score": Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¹Ø§Ù…,
    "level": "Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© (Ù…Ù…ØªØ§Ø²/Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹/Ø¬ÙŠØ¯/Ù…Ù‚Ø¨ÙˆÙ„/ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†/Ø¶Ø¹ÙŠÙ)",
    "strengths": ["Ù†Ù‚Ø·Ø© Ù‚ÙˆØ© 1", "Ù†Ù‚Ø·Ø© Ù‚ÙˆØ© 2", "..."],
    "areas_to_improve": ["Ù…Ø¬Ø§Ù„ Ø§Ù„ØªØ­Ø³ÙŠÙ† 1", "Ù…Ø¬Ø§Ù„ Ø§Ù„ØªØ­Ø³ÙŠÙ† 2", "..."],
    "suggestions": ["Ø§Ù‚ØªØ±Ø§Ø­ 1", "Ø§Ù‚ØªØ±Ø§Ø­ 2", "..."],
    "detailed_analysis": {{
        "pronunciation_notes": "Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø­ÙˆÙ„ Ø§Ù„Ù†Ø·Ù‚",
        "fluency_notes": "Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø­ÙˆÙ„ Ø§Ù„Ø·Ù„Ø§Ù‚Ø©",
        "accuracy_notes": "Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø­ÙˆÙ„ Ø§Ù„Ø¯Ù‚Ø©",
        "comprehension_notes": "Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø­ÙˆÙ„ Ø§Ù„ÙÙ‡Ù…"
    }},
    "encouragement": "Ø±Ø³Ø§Ù„Ø© ØªØ´Ø¬ÙŠØ¹ÙŠØ© Ù„Ù„Ø·Ø§Ù„Ø¨",
    "specific_mistakes": ["Ø®Ø·Ø£ Ù…Ø­Ø¯Ø¯ 1", "Ø®Ø·Ø£ Ù…Ø­Ø¯Ø¯ 2", "..."],
    "improvement_priority": "Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"
}}

ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø¹Ø§Ø¯Ù„ ÙˆÙ…Ø´Ø¬Ø¹ ÙˆÙ…ÙÙŠØ¯ Ù„Ù„Ø·Ø§Ù„Ø¨.
"""
        return prompt
    
    def determine_reading_level(self, overall_score: float) -> ReadingLevel:
        """Determine reading level based on overall score"""
        if overall_score >= 90:
            return ReadingLevel.EXCELLENT
        elif overall_score >= 80:
            return ReadingLevel.VERY_GOOD
        elif overall_score >= 70:
            return ReadingLevel.GOOD
        elif overall_score >= 60:
            return ReadingLevel.ACCEPTABLE
        elif overall_score >= 50:
            return ReadingLevel.NEEDS_IMPROVEMENT
        else:
            return ReadingLevel.POOR
    
    def evaluate_reading(self, transcription: str, original_text: str) -> ReadingEvaluation:
        """
        Evaluate Arabic reading using LLM
        
        Args:
            transcription: The transcribed audio text
            original_text: The original text that should be read
            
        Returns:
            ReadingEvaluation object with detailed assessment
        """
        try:
            # Calculate accuracy metrics first
            accuracy_metrics = self.calculate_accuracy_score(transcription, original_text)
            
            # Create evaluation prompt
            prompt = self.create_evaluation_prompt(transcription, original_text, accuracy_metrics)
            
            # Generate evaluation using Gemini
            response = self.model.generate_content(prompt)
            
            # Extract JSON from response
            response_text = response.text
            
            # Find JSON in the response (sometimes LLM adds extra text)
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_text = response_text[json_start:json_end]
                llm_evaluation = json.loads(json_text)
            else:
                raise ValueError("Could not extract JSON from LLM response")
            
            # Create comprehensive feedback
            comprehensive_feedback = self.create_comprehensive_feedback(
                llm_evaluation, accuracy_metrics, transcription, original_text
            )
            
            # Determine reading level
            overall_score = llm_evaluation.get('overall_score', 0)
            reading_level = self.determine_reading_level(overall_score)
            
            # Create evaluation object
            evaluation = ReadingEvaluation(
                overall_score=overall_score,
                level=reading_level,
                pronunciation_score=llm_evaluation.get('pronunciation_score', 0),
                fluency_score=llm_evaluation.get('fluency_score', 0),
                accuracy_score=llm_evaluation.get('accuracy_score', 0),
                comprehension_score=llm_evaluation.get('comprehension_score', 0),
                feedback=comprehensive_feedback,
                detailed_feedback={
                    'llm_analysis': llm_evaluation,
                    'accuracy_metrics': accuracy_metrics,
                    'reading_statistics': self.calculate_reading_statistics(transcription, original_text)
                },
                suggestions=llm_evaluation.get('suggestions', []),
                strengths=llm_evaluation.get('strengths', []),
                areas_to_improve=llm_evaluation.get('areas_to_improve', [])
            )
            
            return evaluation
            
        except Exception as e:
            # Fallback evaluation if LLM fails
            return self.create_fallback_evaluation(transcription, original_text, str(e))
    
    def create_comprehensive_feedback(self, llm_eval: Dict, accuracy_metrics: Dict, 
                                   transcription: str, original_text: str) -> str:
        """Create comprehensive feedback combining LLM analysis and metrics"""
        
        feedback_parts = []
        
        # Header with overall assessment
        overall_score = llm_eval.get('overall_score', 0)
        level = llm_eval.get('level', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
        
        if overall_score >= 80:
            emoji = "ğŸŒŸ"
            tone = "Ù…Ù…ØªØ§Ø²"
        elif overall_score >= 70:
            emoji = "ğŸ‘"
            tone = "Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹"
        elif overall_score >= 60:
            emoji = "ğŸ‘"
            tone = "Ø¬ÙŠØ¯"
        else:
            emoji = "ğŸ’ª"
            tone = "ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ­Ø³ÙŠÙ†"
        
        feedback_parts.append(f"{emoji} ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©: {level} - {overall_score:.1f}/100")
        feedback_parts.append(f"Ø§Ù„ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ø§Ù…: {tone}")
        
        # Scores breakdown
        feedback_parts.append("\nğŸ“Š ØªÙØµÙŠÙ„ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª:")
        feedback_parts.append(f"â€¢ Ø§Ù„Ù†Ø·Ù‚ ÙˆØ§Ù„ÙˆØ¶ÙˆØ­: {llm_eval.get('pronunciation_score', 0):.1f}/100")
        feedback_parts.append(f"â€¢ Ø§Ù„Ø·Ù„Ø§Ù‚Ø©: {llm_eval.get('fluency_score', 0):.1f}/100")
        feedback_parts.append(f"â€¢ Ø§Ù„Ø¯Ù‚Ø©: {llm_eval.get('accuracy_score', 0):.1f}/100")
        feedback_parts.append(f"â€¢ Ø§Ù„ÙÙ‡Ù…: {llm_eval.get('comprehension_score', 0):.1f}/100")
        
        # Statistical analysis
        feedback_parts.append(f"\nğŸ“ˆ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ:")
        feedback_parts.append(f"â€¢ Ø¯Ù‚Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {accuracy_metrics['word_accuracy']:.1f}%")
        feedback_parts.append(f"â€¢ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©: {accuracy_metrics['correct_words']} Ù…Ù† {accuracy_metrics['total_words_original']}")
        
        if accuracy_metrics['missing_words'] > 0:
            feedback_parts.append(f"â€¢ ÙƒÙ„Ù…Ø§Øª Ù…ÙÙ‚ÙˆØ¯Ø©: {accuracy_metrics['missing_words']}")
        if accuracy_metrics['extra_words'] > 0:
            feedback_parts.append(f"â€¢ ÙƒÙ„Ù…Ø§Øª Ø²Ø§Ø¦Ø¯Ø©: {accuracy_metrics['extra_words']}")
        
        # Strengths
        strengths = llm_eval.get('strengths', [])
        if strengths:
            feedback_parts.append(f"\nâœ… Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ©:")
            for strength in strengths:
                feedback_parts.append(f"â€¢ {strength}")
        
        # Areas to improve
        areas = llm_eval.get('areas_to_improve', [])
        if areas:
            feedback_parts.append(f"\nğŸ¯ Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†:")
            for area in areas:
                feedback_parts.append(f"â€¢ {area}")
        
        # Specific suggestions
        suggestions = llm_eval.get('suggestions', [])
        if suggestions:
            feedback_parts.append(f"\nğŸ’¡ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù„Ù„ØªØ­Ø³ÙŠÙ†:")
            for suggestion in suggestions:
                feedback_parts.append(f"â€¢ {suggestion}")
        
        # Detailed analysis if available
        detailed = llm_eval.get('detailed_analysis', {})
        if detailed:
            feedback_parts.append(f"\nğŸ” ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„:")
            for key, value in detailed.items():
                if value and value.strip():
                    area_name = {
                        'pronunciation_notes': 'Ø§Ù„Ù†Ø·Ù‚',
                        'fluency_notes': 'Ø§Ù„Ø·Ù„Ø§Ù‚Ø©',
                        'accuracy_notes': 'Ø§Ù„Ø¯Ù‚Ø©',
                        'comprehension_notes': 'Ø§Ù„ÙÙ‡Ù…'
                    }.get(key, key)
                    feedback_parts.append(f"â€¢ {area_name}: {value}")
        
        # Encouragement
        encouragement = llm_eval.get('encouragement', '')
        if encouragement:
            feedback_parts.append(f"\nğŸŒŸ {encouragement}")
        
        # Priority improvement
        priority = llm_eval.get('improvement_priority', '')
        if priority:
            feedback_parts.append(f"\nğŸ¯ Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„ØªØ­Ø³ÙŠÙ†: {priority}")
        
        return "\n".join(feedback_parts)
    
    def calculate_reading_statistics(self, transcription: str, original_text: str) -> Dict[str, Any]:
        """Calculate additional reading statistics"""
        
        # Clean texts
        clean_trans = self.normalize_arabic_text(transcription)
        clean_orig = self.normalize_arabic_text(original_text)
        
        # Word counts
        trans_words = clean_trans.split()
        orig_words = clean_orig.split()
        
        # Character counts (Arabic letters only)
        trans_chars = len(self.arabic_letters_pattern.findall(clean_trans))
        orig_chars = len(self.arabic_letters_pattern.findall(clean_orig))
        
        # Calculate reading speed (assuming 1 word per second as baseline)
        estimated_reading_time = len(orig_words) * 1.0  # seconds
        
        return {
            "original_word_count": len(orig_words),
            "transcribed_word_count": len(trans_words),
            "original_char_count": orig_chars,
            "transcribed_char_count": trans_chars,
            "word_completion_rate": (len(trans_words) / len(orig_words) * 100) if orig_words else 0,
            "char_completion_rate": (trans_chars / orig_chars * 100) if orig_chars else 0,
            "estimated_reading_time": estimated_reading_time,
            "words_per_minute": (len(trans_words) / estimated_reading_time * 60) if estimated_reading_time > 0 else 0
        }
    
    def create_fallback_evaluation(self, transcription: str, original_text: str, error_msg: str) -> ReadingEvaluation:
        """Create a basic evaluation when LLM fails"""
        
        accuracy_metrics = self.calculate_accuracy_score(transcription, original_text)
        basic_score = accuracy_metrics['word_accuracy']
        
        # Simple level determination
        if basic_score >= 90:
            level = ReadingLevel.EXCELLENT
        elif basic_score >= 70:
            level = ReadingLevel.GOOD
        elif basic_score >= 50:
            level = ReadingLevel.ACCEPTABLE
        else:
            level = ReadingLevel.NEEDS_IMPROVEMENT
        
        fallback_feedback = f"""
âš ï¸ ØªÙ‚ÙŠÙŠÙ… Ø£Ø³Ø§Ø³ÙŠ (Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…)

ğŸ“Š Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {basic_score:.1f}/100
ğŸ“ˆ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©: {level.value}

ğŸ“ˆ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ:
â€¢ Ø¯Ù‚Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {accuracy_metrics['word_accuracy']:.1f}%
â€¢ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©: {accuracy_metrics['correct_words']} Ù…Ù† {accuracy_metrics['total_words_original']}
â€¢ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©: {accuracy_metrics['missing_words']}
â€¢ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©: {accuracy_metrics['extra_words']}

ğŸ’¡ Ù†ØµØ§Ø¦Ø­ Ø¹Ø§Ù…Ø©:
â€¢ ØªØ¯Ø±Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ø¨ØµÙˆØª Ø¹Ø§Ù„Ù
â€¢ Ø§Ù‚Ø±Ø£ Ø¨Ø¨Ø·Ø¡ ÙˆÙˆØ¶ÙˆØ­
â€¢ Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù†Øµ Ù‚Ø¨Ù„ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©
â€¢ ØªØ¯Ø±Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø·Ù‚ Ø§Ù„ØµØ­ÙŠØ­

ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø®Ø·Ø£: {error_msg}
"""
        
        return ReadingEvaluation(
            overall_score=basic_score,
            level=level,
            pronunciation_score=basic_score,
            fluency_score=basic_score,
            accuracy_score=basic_score,
            comprehension_score=basic_score,
            feedback=fallback_feedback,
            detailed_feedback={
                'accuracy_metrics': accuracy_metrics,
                'error': error_msg
            },
            suggestions=["ØªØ¯Ø±Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ø¨Ø§Ù†ØªØ¸Ø§Ù…", "Ø§Ù‚Ø±Ø£ Ø¨Ø¨Ø·Ø¡ ÙˆÙˆØ¶ÙˆØ­", "Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù†Øµ Ù‚Ø¨Ù„ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©"],
            strengths=["Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬ÙŠØ¯Ø© Ù„Ù„Ù‚Ø±Ø§Ø¡Ø©"],
            areas_to_improve=["Ø¯Ù‚Ø© Ø§Ù„Ù†Ø·Ù‚", "Ø·Ù„Ø§Ù‚Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©"]
        )
